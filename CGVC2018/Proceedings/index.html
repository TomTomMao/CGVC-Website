<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta name="generator" content=
  "HTML Tidy for Windows (vers 7 December 2008), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link rel="stylesheet" type="text/css" href="images/dvd.css">
  <script type="text/javascript">
              function myFunction(n) {
                    var id = n
                    var x = document.getElementById(id);
                    if (x.style.display === 'block') {
                        x.style.display = 'none';
                    } else {
                        x.style.display = 'block';
                    }
                }
  </script>

  <title>Computer Graphics &amp; Visual Computing (CGVC) 2018</title>
</head>

<body>
  <div class="table_welcome">
    <div style="margin-left: 30px;width: 90%">
      <h2>Digital Conference Proceedings</h2>
      <hr>

      <div align="right">
        <h2>Computer Graphics &amp; Visual Computing (CGVC) 2018</h2>

        <h3>Swansea - UK | September 2018</h3>
      </div>
    </div>
  </div>

  <div class="table2">
    <div class="bibtex">
      <a href="bibtex.bib" target="_blank">Download BiBTeX (whole event)</a>
    </div>

    <div style="clear:both;"></div>

    <div class="other">
      <br>
      <a href="000_cgvc2018_frontmatter.pdf">Table of Contents</a>
    </div>

    <div class="sectionheader">
      Vision and Learning
    </div>

    <div class="title">
      Topological Connected Chain Modelling for Classification of Mammographic
      Microcalcification
    </div>

    <div class="links">
      [<a href="pdf/001-005.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="001-005.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Minu George, Erika R. E. Denton, and Reyer Zwiggelaar
    </div>

    <div class="button">
      <button onclick="myFunction(1)">+/-</button>
    </div>

    <div id='1' class="abstract">
      Breast cancer continues to be the most common type of cancer among women.
      Early detection of breast cancer is key to effective treatment. The
      presence of clusters of fine, granular microcalcifications in
      mammographic images can be a primary sign of breast cancer. The
      malignancy of any cluster of microcalcification cannot be reliably
      determined by radiologists from mammographic images and need to be
      assessed through histology images. In this paper, a novel method of
      mammographic microcalcification classification is described using the
      local topological structure of microcalcifications. Unlike the
      statistical and texture features of microcalcifications, the proposed
      method focuses on the number of microcalcifications in local clusters,
      the distance between them, and the number of clusters. The initial
      evaluation on the Digital Database for Screening Mammography (DDSM)
      database shows promising results with 86% accuracy and findings which are
      in line with clinical perception of benign and malignant morphological
      appearance of microcalcification clusters.
    </div>

    <div class="title">
      Combining Accumulated Frame Differencing and Corner Detection for Motion
      Detection
    </div>

    <div class="links">
      [<a href="pdf/007-014.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="007-014.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Nahlah Algethami and Sam Redfern
    </div>

    <div class="button">
      <button onclick="myFunction(2)">+/-</button>
    </div>

    <div id='2' class="abstract">
      Detecting and tracking people in a meeting room is very important for
      many applications. In order to detect people in a meeting room with no
      prior knowledge (e.g. background model) and regardless of whether their
      motion is slow or significant, this paper proposes a coarse-to-fine
      people detection algorithm by combining a novel motion detection process,
      namely, adaptive accumulated frame differencing (AAFD) combined with
      corner features. Firstly, the region of movement is extracted adaptively
      using AAFD, then motion corner features are extracted. Finally, the
      minimum area rectangle fitting these corners is found. The proposed
      algorithm is evaluated using the AMI meeting data set and this indicates
      promising results for people detection.
    </div>

    <div class="title">
      Groupwise Non-rigid Image Alignment With Graph-based Initialisation
    </div>

    <div class="links">
      [<a href="pdf/015-021.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="015-021.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Ahmad Aal-Yhia, Paul Malcolm, Otar Akanyeti, Reyer Zwiggelaar, and
      Bernard Tiddeman
    </div>

    <div class="button">
      <button onclick="myFunction(3)">+/-</button>
    </div>

    <div id='3' class="abstract">
      Groupwise image alignment automatically provides non-rigid registration
      across a set of images. It has found applications in facial image
      analysis and medical image analysis by automatically generating
      statistical models of shape and appearance. The main approaches used
      previously include iterative and graph-based approaches. In iterative
      approaches, the registration of each image is iteratively updated to
      minimise an error measure across the set. Various metrics and
      optimisation strategies have been proposed to achieve this. Graph-based
      methods perform registration of each pair of images in the set, to form a
      weighted graph of the ''distance'' between all the images, and then finds
      the optimal paths between the most central image and every other image.
      In this paper, we use a graph-based approach to perform initialisation,
      which is then refined with an iterative approach. Pairwise registration
      is performed using demons registration, then shortest paths identified in
      the resulting graph are used to provide an initial warp for each image by
      concatenating warps along the path. The warps are refined using an
      iterative Levenberg-Marquardt minimisation to the mean, based on updating
      the locations of a small number of points and incorporating a stiffness
      constraint. This optimisation approach is efficient, has very few free
      parameters to tune and we show how to tune the few remaining parameters.
      We compare the combined approach to both the iterative and graph-based
      approaches used independently. Results demonstrate that the combined
      method improves the alignment of various datasets, including two face
      datasets and a difficult medical dataset of prostate MRI images.
    </div>

    <div class="title">
      A Deep Learning Approach to No-Reference Image Quality Assessment For
      Monte Carlo Rendered Images
    </div>

    <div class="links">
      [<a href="pdf/023-031.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="023-031.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Joss Whittle and Mark W. Jones
    </div>

    <div class="button">
      <button onclick="myFunction(4)">+/-</button>
    </div>

    <div id='4' class="abstract">
      In Full-Reference Image Quality Assessment (FR-IQA) images are compared
      with ground truth images that are known to be of high visual quality.
      These metrics are utilized in order to rank algorithms under test on
      their image quality performance. Throughout the progress of Monte Carlo
      rendering processes we often wish to determine whether images being
      rendered are of sufficient visual quality, without the availability of a
      ground truth image. In such cases FR-IQA metrics are not applicable and
      we instead must utilise No-Reference Image Quality Assessment (NR-IQA)
      measures to make predictions about the perceived quality of unconverged
      images. In this work we propose a deep learning approach to NR-IQA,
      trained specifically on noise from Monte Carlo rendering processes, which
      significantly outperforms existing NR-IQA methods and can produce quality
      predictions consistent with FR-IQA measures that have access to ground
      truth images.
    </div>

    <div class="sectionheader">
      Graphics
    </div>

    <div class="title">
      Keys-to-Sim: Transferring Hand-Crafted Key-framed Animations to Simulated
      Figures using Wide Band Stochastic Trajectory Optimization
    </div>

    <div class="links">
      [<a href="pdf/033-041.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="033-041.pdf.html">meta data
      <img src="images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="images/movie.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Dominik Borer, Martin Guay, and Robert W. Sumner
    </div>

    <div class="button">
      <button onclick="myFunction(5)">+/-</button>
    </div>

    <div id='5' class="abstract">
      The vision of fully simulating characters and their environments has the
      potential to offer rich interactions between characters and objects in
      the virtual world. However, this introduces a challenging problem similar
      to controlling robotic figures: computing the necessary torques to
      perform a given task. In this paper, we address the problem of
      transferring hand-crafted kinematic motions to a fully simulated figure,
      by computing open-loop controls necessary to reproduce the target motion.
      One key ingredient to successful control is the mechanical feasibility of
      the target motion. While several methods have been successful at
      replicating human captured motion, there has not yet been a method
      capable of handling the case of artist-authored key-framed movements that
      can violate the laws of physics or go beyond the mechanical limits of the
      character. Due to the curse of dimensionality, sampling-based
      optimization methods typically restrict the search to a narrow band which
      limits exploration of feasible motions&mdash;resulting in a failure to
      reproduce the desired motion when a large deviation is required. In this
      paper, we solve this problem by combining a window-based breakdown of the
      controls on the temporal dimension, together with a global wide search
      strategy that keeps locally sub-optimal samples throughout the
      optimization.
    </div>

    <div class="title">
      Image Based Proximate Shadow Retargeting
    </div>

    <div class="links">
      [<a href="pdf/043-050.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="043-050.pdf.html">meta data
      <img src="images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="images/movie.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Llogari Casas, Matthias Fauconneau, Maggie Kosek, Kieran Mclister, and
      Kenny Mitchell
    </div>

    <div class="button">
      <button onclick="myFunction(6)">+/-</button>
    </div>

    <div id='6' class="abstract">
      We introduce Shadow Retargeting which maps real shadow appearance to
      virtual shadows given a corresponding deformation of scene geometry, such
      that appearance is seamlessly maintained. By performing virtual shadow
      reconstruction from un-occluded real shadow samples observed in the
      camera frame, we recover the deformed shadow appearance efficiently. Our
      method uses geometry priors for the shadow casting object and a planar
      receiver surface. Inspired by image retargeting approaches [VTP10] we
      describe a novel local search strategy, steered by importance based
      deformed shadow estimation. Results are presented on a range of objects,
      deformations and illumination conditions in real-time Augmented Reality
      (AR) on a mobile device. We demonstrate the practical application of the
      method in generating otherwise laborious in-betweening frames for 3D
      printed stop motion animation.
    </div>

    <div class="title">
      Physically-based Sticky Lips
    </div>

    <div class="links">
      [<a href="pdf/051-059.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="051-059.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Matthew Leach and Steve Maddock
    </div>

    <div class="button">
      <button onclick="myFunction(7)">+/-</button>
    </div>

    <div id='7' class="abstract">
      Abstract In this paper, a novel solution is provided for the sticky lip
      problem in computer facial animation, recreating the way the lips stick
      together when drawn apart in speech or in the formation of facial
      expressions. Traditional approaches to modelling this rely on an artist
      estimating the correct behaviour. In contrast, this paper presents a
      physically-based model. The mouth is modelled using the total Lagrangian
      explicit dynamics finite element method, with a new breaking element
      modelling the saliva between the lips. With this approach, subtle yet
      complex behaviours are recreated implicitly, giving rise to more
      realistic movements of the lips. The model is capable of reproducing
      varying degrees of stickiness between the lips, as well as asymmetric
      effects.
    </div>

    <div class="title">
      Screen Space Particle Selection
    </div>

    <div class="links">
      [<a href="pdf/061-069.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="061-069.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Marcel K&ouml;ster and Antonio Kr&uuml;ger
    </div>

    <div class="button">
      <button onclick="myFunction(8)">+/-</button>
    </div>

    <div id='8' class="abstract">
      Analyses of large 3D particle datasets typically involve many different
      exploration and visualization steps. Interactive exploration techniques
      are essential to reveal and select interesting subsets like clusters or
      other sophisticated structures. State-of-the-art techniques allow for
      context-aware selections that can be refined dynamically. However, these
      techniques require large amounts of memory and have high computational
      complexity which heavily limits their applicability to large datasets. We
      propose a novel, massively parallel particle selection method that is
      easy to implement and has a processing complexity of O(n*k) (where n is
      the number of particles and k the maximum number of neighbors per
      particle) and requires only O(n) memory. Furthermore, our algorithm is
      designed for GPUs and performs a selection step in several milliseconds
      while still being able to achieve high-quality results.
    </div>

    <div class="sectionheader">
      Visualization I
    </div>

    <div class="title">
      GPU-Assisted Scatterplots for Millions of Call Events
    </div>

    <div class="links">
      [<a href="pdf/071-079.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="071-079.pdf.html">meta data
      <img src="images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="images/movie.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Dylan Rees, Richard C. Roberts, Robert S. Laramee, Paul Brookes, Tony
      D'Cruze, and Gary A. Smith
    </div>

    <div class="button">
      <button onclick="myFunction(9)">+/-</button>
    </div>

    <div id='9' class="abstract">
      With four percent of the working population employed in call centers in
      both the United States and the UK, the contact center industry represents
      a sizable proportion of modern industrial landscapes. As with most modern
      industries, data collection is de rigueur, producing gigabytes of call
      records that require analysis. The scatterplot is a well established and
      understood form of data visualization dating back to the 17th century. In
      this paper we present an application for visualizing large call centre
      data sets using hardware-accelerated scatterplots. The application
      utilizes a commodity graphics card to enable visualization of a month's
      worth of data, enabling fast filtering of multiple attributes. Filtering
      is implemented using the Open Computing Language (OpenCL), providing
      significant performance improvement over traditional methods. We
      demonstrate the value of our application for exploration and analysis of
      millions of call events from a real-world industry partner. Domain expert
      feedback from our industrial partners is reported.
    </div>

    <div class="title">
      RiverState: A Visual Metaphor Representing Millions of Time-Oriented
      State Transitions
    </div>

    <div class="links">
      [<a href="pdf/081-089.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="081-089.pdf.html">meta data
      <img src="images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="images/movie.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Richard C. Roberts, Dylan Rees, Robert S. Laramee, Paul Brookes, and Gary A.
      Smith
    </div>

    <div class="button">
      <button onclick="myFunction(10)">+/-</button>
    </div>

    <div id='10' class="abstract">
      Developing a positive relationship between a business and its customers
      is vital to success. The outcome of any customer interaction can
      determine future patronage of the business. Many industry's only point of
      interaction with their customers is through a contact centre where
      everything from sales to complaints are handled. This places tremendous
      importance on the operational efficiency of the contact centre and the
      level of care provided to the customers. These customer interactions are
      recorded and archived in large databases, but undertaking insightful
      analysis is challenging due to both the size and complexity of the data.
      We present a visual solution to the tracking of customer interactions at
      a large scale. RiverState visualises the collective flow of callers
      through the process of interacting with a contact centre using a river
      metaphor. We use finite state transition machines with customised edges
      to depict millions of events and the states callers go through to
      complete their journey. We implement a range of novel features to enhance
      the analytical qualities of the application, and collect feedback from
      domain experts to analyse and evaluate the use of the software.
    </div>

    <div class="title">
      Towards a Survey of Interactive Visualization for Education
    </div>

    <div class="links">
      [<a href="pdf/091-101.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="091-101.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Elif E. F&#305;rat and Robert S. Laramee
    </div>

    <div class="button">
      <button onclick="myFunction(11)">+/-</button>
    </div>

    <div id='11' class="abstract">
      Graphic design and visualization are becoming fundamental components of
      education. The use of advanced visual design in pedagogy is growing and
      evolving rapidly. One of their aims is to enhance the educational process
      by facilitating better understanding of the subject with the use of
      graphical representation methods. Research papers in this field offer
      important opportunities to examine previously completed experiments and
      extract useful educational outcomes. This paper analyzes and classifies
      pedagogical visualization research papers to increase understanding in
      this area. To our knowledge, this is the first (work-in-progress) survey
      paper on advanced visualization for education. We categorize related
      research papers into original subject groups that enable researchers to
      compare related literature. Our novel classification enables researchers
      to find both mature and unexplored directions which can inform directions
      for future work. This paper serves as a valuable resource for both
      beginners and experienced researchers who are interested in interactive
      visualization for education.
    </div>

    <div class="sectionheader">
      Short Papers
    </div>

    <div class="title">
      Image Inpainting for High-Resolution Textures using CNN Texture Synthesis
    </div>

    <div class="links">
      [<a href="pdf/103-107.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="103-107.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Pascal Laube, Michael Grunwald, Matthias O. Franz, and Georg Umlauf
    </div>

    <div class="button">
      <button onclick="myFunction(12)">+/-</button>
    </div>

    <div id='12' class="abstract">
      Deep neural networks have been successfully applied to problems such as
      image segmentation, image super-resolution, coloration and image
      inpainting. In this work we propose the use of convolutional neural
      networks (CNN) for image inpainting of large regions in high-resolution
      textures. Due to limited computational resources processing
      high-resolution images with neural networks is still an open problem.
      Existing methods separate inpainting of global structure and the transfer
      of details, which leads to blurry results and loss of global coherence in
      the detail transfer step. Based on advances in texture synthesis using
      CNNs we propose patch-based image inpainting by a CNN that is able to
      optimize for global as well as detail texture statistics. Our method is
      capable of filling large inpainting regions, oftentimes exceeding the
      quality of comparable methods for high-resolution images. For reference
      patch look-up we propose to use the same summary statistics that are used
      in the inpainting process.
    </div>

    <div class="title">
      Segmenting Teeth from Volumetric CT Data with a Hierarchical CNN-based
      Approach
    </div>

    <div class="links">
      [<a href="pdf/109-113.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="109-113.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Philipp Marten Macho, Nadja Kurz, Adrian Ulges, Robert Brylka, Thomas
      Gietzen, and Ulrich Schwanecke
    </div>

    <div class="button">
      <button onclick="myFunction(13)">+/-</button>
    </div>

    <div id='13' class="abstract">
      This paper addresses the automatic segmentation of teeth in volumetric
      Computed Tomography (CT) scans of the human skull. Our approach is based
      on a convolutional neural network employing 3D volumetric convolutions.
      To tackle data scale issues, we apply a hierarchical coarse-to fine
      approach combining two CNNs, one for low-resolution detection and one for
      highresolution refinement. In quantitative experiments on 40 CT scans
      with manually acquired ground truth, we demonstrate that our approach
      displays remarkable robustness across different patients and device
      vendors. Furthermore, our hierarchical extension outperforms a
      single-scale segmentation, and network size can be reduced compared to
      previous architectures without loss of accuracy.
    </div>

    <div class="title">
      Voronoi Tree Maps with Circular Boundaries
    </div>

    <div class="links">
      [<a href="pdf/115-116.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="115-116.pdf.html">meta data
      <img src="images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="images/movie.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Abdalla G. M. Ahmed
    </div>

    <div class="button">
      <button onclick="myFunction(14)">+/-</button>
    </div>

    <div id='14' class="abstract">
      Voronoi tree maps are an important milestone in information
      visualization, representing a substantial advancement of the original
      tree maps concept. We address a less-studied variant of Voronoi tree maps
      that uses multiplicative-weighted Voronoi diagrams. We highlight the
      merits of this variant, and discuss the difficulties that might have
      discouraged further exploration, proposing insights for overcoming these
      difficulties.
    </div>

    <div class="title">
      Single ImageWatermark Retrieval from 3D Printed Surfaces via
      Convolutional Neural Networks
    </div>

    <div class="links">
      [<a href="pdf/117-120.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="117-120.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Xin Zhang, Qian Wang, and Ioannis Ivrissimtzis
    </div>

    <div class="button">
      <button onclick="myFunction(15)">+/-</button>
    </div>

    <div id='15' class="abstract">
      In this paper we propose and analyse a method for watermarking 3D printed
      objects, concentrating on the watermark retrieval problem. The method
      embeds the watermark in a planar region of the 3D printed object in the
      form of small semi-spherical or cubic bumps arranged at the nodes of a
      regular grid. The watermark is extracted from a single image of the
      watermarked planar region through a Convolutional Neural Network.
      Experiments with 3D printed objects, produced by filaments of various
      colours, show that in most cases the retrieval method has a high accuracy
      rate.
    </div>

    <div class="title">
      Evolutionary Interactive Analysis of MRI Gastric Images Using a
      Multiobjective Cooperative-coevolution Scheme
    </div>

    <div class="links">
      [<a href="pdf/121-125.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="121-125.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Shatha F. Al-Maliki, &Eacute;velyne Lutton, Fran&ccedil;ois Bou&eacute;,
      and Franck P. Vidal
    </div>

    <div class="button">
      <button onclick="myFunction(16)">+/-</button>
    </div>

    <div id='16' class="abstract">
      In this study, we combine computer vision and visualisation/data
      exploration to analyse magnetic resonance imaging (MRI) data and detect
      garden peas inside the stomach. It is a preliminary objective of a larger
      project that aims to understand the kinetics of gastric emptying. We
      propose to perform the image analysis task as a multi-objective
      optimisation. A set of 7 equally important objectives are proposed to
      characterise peas. We rely on a cooperation co-evolution algorithm called
      'Fly Algorithm' implemented using NSGA-II. The Fly Algorithm is a
      specific case of the 'Parisian Approach' where the solution of an
      optimisation problem is represented as a set of individuals (e.g. the
      whole population) instead of a single individual (the best one) as in
      typical evolutionary algorithms (EAs). NSGA-II is a popular EA used to
      solve multi-objective optimisation problems. The output of the
      optimisation is a succession of datasets that progressively approximate
      the Pareto front, which needs to be understood and explored by the
      end-user. Using interactive Information Visualisation (InfoVis) and
      clustering techniques, peas are then semi-automatically segmented.
    </div>

    <div class="sectionheader">
      Visualization II
    </div>

    <div class="title">
      Cartograms with Topological Features
    </div>

    <div class="links">
      [<a href="pdf/127-134.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="127-134.pdf.html">meta data
      <img src="images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="images/movie.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Chao Tong, Liam McNabb, and Robert S. Laramee
    </div>

    <div class="button">
      <button onclick="myFunction(17)">+/-</button>
    </div>

    <div id='17' class="abstract">
      Cartograms are a popular and useful technique for depicting geo-spatial
      data. Dorling style and rectangular cartograms are very good for
      facilitating comparisons between unit areas. Each unit area is
      represented by the same shape such as a circle or rectangle, and the
      uniformity in shapes facilitates comparative judgment. However, the
      layout of these more abstract shapes may also simultaneously reduce the
      map's legibility and increase error. When we integrate univariate data
      into a cartogram, the recognizability of cartogram may be reduced. There
      is a trade-off between information recognition and geo-information
      accuracy. This is the inspiration behind the work we present. We attempt
      to increase the map's recognizability and reduce error by introducing
      topological features into the cartographic map. Our goal is to include
      topological features such as a river in a Dorling-style or rectangular
      cartogram to make the visual layout more recognizable, increase map
      cognition and reduce geospatial error. We believe that compared to the
      standard Dorling and rectangular style cartogram, adding topological
      features provides familiar geo-spatial cues and flexibility to enhance
      the recognizability of a cartogram.
    </div>

    <div class="title">
      Spectrum: A C++ Header Library for Colour Map Management
    </div>

    <div class="links">
      [<a href="pdf/135-141.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="135-141.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Richard C. Roberts, Liam McNabb, Naif AlHarbi, and Robert S. Laramee
    </div>

    <div class="button">
      <button onclick="myFunction(18)">+/-</button>
    </div>

    <div id='18' class="abstract">
      The use of colour mapping is fundamental to visualisation research. It
      acts as an additional layer beyond rendering in the spatial dimensions
      and provides a link between values in any dataset. When designing and
      building visualisation research software, the process of creating and
      managing a colour mapping system can be time-consuming and complex.
      Existing alternatives offer niche features and require complex
      dependencies or installations. We present Spectrum; an open source colour
      map management library that is developer friendly with no installation
      required, and that offers a wide variety of features for the majority of
      use cases. We demonstrate the utility of the library through simple
      snippets of code and a number of examples which illustrate its ease of
      use and functionality, as well as a video demonstrating the installation
      and use of the library in under two minutes. It is a very valuable
      jump-start tool for developers and researchers who need to focus on other
      tasks.
    </div>

    <div class="title">
      SoS TextVis: A Survey of Surveys on Text Visualization
    </div>

    <div class="links">
      [<a href="pdf/143-152.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="143-152.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Mohammad Alharbi and Robert S. Laramee
    </div>

    <div class="button">
      <button onclick="myFunction(19)">+/-</button>
    </div>

    <div id='19' class="abstract">
      Text visualization is a rapidly growing sub-field of information
      visualization and visual analytics. There are many approaches and
      techniques introduced every year to address a wide range of tasks and
      enable researchers from different disciplines to obtain leading-edge
      knowledge from digitized collections. This can be challenging
      particularly when the data is massive. Additionally, the sources of
      digital text have spread substantially in the last decades in various
      forms, such as web pages, blogs, twitter, email, electronic publications,
      and books. In response to the explosion of text visualization research
      literature, the first survey article was published in 2010. Furthermore,
      there are a growing number of surveys that review existing techniques and
      classify them based on text research methodology. In this work, we aim to
      present the first Survey of Surveys (SoS) that review all of the survey
      and state-of-the-art papers on text visualization techniques and provide
      an SoS classification. We study and compare the surveys, and categorize
      them into 5 groups: (1) document-centered, (2) user task analysis, (3)
      cross-disciplinary, (4) multifaceted, and (5) satellite-themed. We
      provide survey recommendations for researchers in the field of text
      visualization. The result is a very unique, valuable starting point and
      overview of the current state-of-the-art in text visualization research
      literature.
    </div>

    <div class="sectionheader">
      Visualization III and VR
    </div>

    <div class="title">
      Knowledge-based Discovery of Transportation Object Properties by Fusing
      Multi-modal GIS Data
    </div>

    <div class="links">
      [<a href="pdf/153-161.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="153-161.pdf.html">meta data
      <img src="images/metadata.gif" border="0" height="16" width="16">]</a>
    </div><a href="153-161.pdf.html"></a>

    <div class="author">
      Pedro Eid Maroun, Sudhir Mudur, and Tiberiu Popa
    </div>

    <div class="button">
      <button onclick="myFunction(20)">+/-</button>
    </div>

    <div id='20' class="abstract">
      3D models of transportation objects like a road, bridge, underpass, etc.
      are required in many domains including military training, land
      development, etc. While remote sensed images and LiDaR data can be used
      to create approximate 3D representations, detailed 3D representations are
      difficult to create automatically. Instead, interactive tools are used
      with rather laborious effort. For example, the top commercial interactive
      model generator we tried required 94 parameters in all for different
      bridge types. In this paper, we take a different path.We automatically
      derive these parameter values from GIS (Geographic Information Systems)
      data, which normally contains detailed information of these objects, but
      often only implicitly. The framework presented here transforms GIS data
      into a knowledge base consisting of assertions. Spatial/numeric relations
      are handled through plug-ins called property extractors whose results get
      added to the knowledge base, used by a reasoning engine to infer object
      properties. A number of properties have to be extracted from images, and
      are dependent on the accuracy of computer vision methods. While a
      comprehensive property extractor mechanism is work in progress, . a
      prototype implementation illustrates our framework for bridges with GIS
      data from the real world. To the best of our knowledge, our framework is
      the first to integrate knowledge inference and uncertainty for extracting
      landscape object properties by fusing facts from multi-modal GIS data
      sources.
    </div>

    <div class="title">
      When Size Matters: Towards Evaluating Perceivability of Choropleths
    </div>

    <div class="links">
      [<a href="pdf/163-171.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="163-171.pdf.html">meta data
      <img src="images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="images/movie.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Liam McNabb, Robert S. Laramee, and Max Wilson
    </div>

    <div class="button">
      <button onclick="myFunction(21)">+/-</button>
    </div>

    <div id='21' class="abstract">
      Choropleth maps are an invaluable visualization type for mapping
      geo-spatial data. One advantage to a choropleth map over other geospatial
      visualizations such as cartograms is the familiarity of a non-distorted
      landmass. However, this causes challenges when an area becomes too small
      in order to accurately perceive the underlying color. When does size
      matter in a choropleth map? We conduct an experiment to verify the
      relationship between choropleth maps, their underlying color map, and a
      user's perceivability. We do this by testing a user's perception of color
      relative to an administrative area's size within a choropleth map, as
      well as user-preference of fixed-locale maps with enforced minimum areas.
      Based on this initial experiment we can make the first recommendations
      with respect to a unit area's minimum size in order to be perceivably
      useful.
    </div>

    <div class="title">
      Virtual Reality: A Literature Review and Metrics-based Classification
    </div>

    <div class="links">
      [<a href="pdf/173-181.pdf">full paper</a> <img src="images/pdf.gif"
      border="0" height="16" width="16">] [<a href="173-181.pdf.html">meta
      data</a> <img src="images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Peter Ankomah and Peter Vangorp
    </div>

    <div class="button">
      <button onclick="myFunction(22)">+/-</button>
    </div>

    <div id='22' class="abstract">
      This paper presents a multi-disciplinary overview of research evaluating
      virtual reality (VR). The main aim is to review and classify VR research
      based on several metrics: presence and immersion, navigation and
      interaction, knowledge improvement, performance and usability. With the
      continuous development and consumerisation of VR, several application
      domains have studied the impact of VR as an enhanced alternative
      environment for performing tasks. However, VR experiment results often
      cannot be generalised but require specific datasets and tasks suited to
      each domain. This review and classification of VR metrics presents an
      alternative metrics-based view of VR experiments and research.
    </div>
  </div>
</body>
</html>
